# ü§ñ Instru√ß√µes: Configura√ß√£o do Modelo Phi-3-mini-4k-instruct-gguf

## üì• Download do Modelo

### Op√ß√£o 1: Download Direto do HuggingFace (Recomendado)

1. Acesse: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf

2. Escolha uma das vers√µes quantizadas (mais leves e r√°pidas):
   - **Phi-3-mini-4k-instruct-q4.gguf** (Recomendado para CPU) - ~2.4GB
   - **Phi-3-mini-4k-instruct-fp16.gguf** (Maior qualidade) - ~7.6GB
   - **Phi-3-mini-4k-instruct-q8.gguf** (Balan√ßo qualidade/tamanho) - ~4.1GB

3. Clique no arquivo desejado e depois em "Download"

### Op√ß√£o 2: Download via CLI

```bash
# Instale o Hugging Face CLI
pip install huggingface_hub

# Baixe o modelo
huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf \
  Phi-3-mini-4k-instruct-q4.gguf \
  --local-dir ./ModelosIA \
  --local-dir-use-symlinks False
```

## üìÇ Instala√ß√£o

### 1. Criar Pasta de Modelos

Na raiz do projeto da API, crie a pasta `ModelosIA`:

```bash
cd Cardapio_Inteligente.Api
mkdir ModelosIA
```

### 2. Mover o Modelo

Copie o arquivo `.gguf` baixado para a pasta `ModelosIA`:

```bash
# Windows (PowerShell)
Copy-Item "C:\Users\SeuUsuario\Downloads\Phi-3-mini-4k-instruct-q4.gguf" -Destination ".\ModelosIA\"

# Linux/Mac
cp ~/Downloads/Phi-3-mini-4k-instruct-q4.gguf ./ModelosIA/
```

### 3. Verificar Estrutura de Pastas

Sua estrutura deve ficar assim:

```
Cardapio_Inteligente.Api/
‚îú‚îÄ‚îÄ Controllers/
‚îú‚îÄ‚îÄ Dados/
‚îú‚îÄ‚îÄ ModelosIA/                              ‚Üê NOVA PASTA
‚îÇ   ‚îî‚îÄ‚îÄ Phi-3-mini-4k-instruct-q4.gguf     ‚Üê MODELO AQUI
‚îú‚îÄ‚îÄ Modelos/
‚îú‚îÄ‚îÄ Servicos/
‚îú‚îÄ‚îÄ appsettings.json
‚îú‚îÄ‚îÄ Program.cs
‚îî‚îÄ‚îÄ ...
```

## ‚öôÔ∏è Configura√ß√£o

### 1. Verificar appsettings.json

O arquivo `appsettings.json` j√° est√° configurado:

```json
{
  "LlamaSettings": {
    "ModelPath": "ModelosIA/Phi-3-mini-4k-instruct-q4.gguf",
    "MaxTokens": 512,
    "Temperature": 0.8,
    "TopP": 0.9,
    "GpuLayerCount": 0,
    "NumThreads": 4,
    "ContextSize": 4096
  }
}
```

### 2. Ajustar para GPU (Opcional)

Se voc√™ tem GPU NVIDIA com CUDA:

```json
{
  "LlamaSettings": {
    "ModelPath": "ModelosIA/Phi-3-mini-4k-instruct-q4.gguf",
    "MaxTokens": 512,
    "Temperature": 0.8,
    "TopP": 0.9,
    "GpuLayerCount": 32,  ‚Üê ALTERE PARA 32 ou mais
    "NumThreads": 4,
    "ContextSize": 4096
  }
}
```

## üöÄ Execu√ß√£o

### 1. Restaurar Pacotes

```bash
cd Cardapio_Inteligente.Api
dotnet restore
```

### 2. Executar a API

```bash
dotnet run
```

### 3. Verificar Log de Inicializa√ß√£o

Voc√™ deve ver no console:

```
üîÑ Carregando modelo Phi-3-mini de: C:\...\ModelosIA\Phi-3-mini-4k-instruct-q4.gguf
‚úÖ Modelo carregado com sucesso!
```

## ‚ö†Ô∏è Troubleshooting

### Erro: "Modelo n√£o encontrado"

```
‚ùå Modelo n√£o encontrado: ...\ModelosIA\Phi-3-mini-4k-instruct-q4.gguf
```

**Solu√ß√£o:**
- Verifique se o arquivo est√° na pasta `ModelosIA`
- Confirme se o nome do arquivo no `appsettings.json` est√° correto
- Certifique-se de que a extens√£o √© `.gguf`

### Erro: "Out of Memory"

**Solu√ß√£o:**
- Use uma vers√£o mais leve (q4 em vez de fp16)
- Reduza `ContextSize` no appsettings.json:
  ```json
  "ContextSize": 2048  // ou menor
  ```

### Performance Lenta

**Solu√ß√£o CPU:**
```json
"NumThreads": 8  // Aumente baseado nos cores da sua CPU
```

**Solu√ß√£o GPU:**
```json
"GpuLayerCount": 32  // Ativa acelera√ß√£o GPU (requer CUDA)
```

## üß™ Teste

### Testar via API

```bash
# Usando curl
curl -X POST "http://localhost:5068/api/Pratos/assistente-chat" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer SEU_TOKEN" \
  -d '{
    "Prompt": "Quais pratos sem lactose voc√™s t√™m?"
  }'
```

### Testar via MAUI App

1. Execute a API
2. Execute o app MAUI
3. Fa√ßa login
4. Acesse o chat
5. Pergunte sobre pratos sem lactose

## üìä Especifica√ß√µes do Modelo

- **Nome:** Phi-3-mini-4k-instruct
- **Desenvolvedor:** Microsoft
- **Contexto:** 4096 tokens
- **Par√¢metros:** 3.8B
- **Formato:** GGUF (otimizado para llama.cpp)
- **Linguagens:** Portugu√™s, Ingl√™s, e mais

## üîó Links √öteis

- HuggingFace: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf
- Documenta√ß√£o Phi-3: https://azure.microsoft.com/en-us/products/phi-3/
- LLamaSharp (lib usada): https://github.com/SciSharp/LLamaSharp

## ‚úÖ Checklist de Configura√ß√£o

- [ ] Modelo baixado do HuggingFace
- [ ] Arquivo `.gguf` na pasta `ModelosIA`
- [ ] `appsettings.json` configurado corretamente
- [ ] API inicia sem erros
- [ ] Log mostra "Modelo carregado com sucesso"
- [ ] Chat responde perguntas via API
- [ ] App MAUI conecta e recebe respostas

---

**√öltima atualiza√ß√£o:** 2025
**Vers√£o do Modelo:** Phi-3-mini-4k-instruct-gguf (q4)
